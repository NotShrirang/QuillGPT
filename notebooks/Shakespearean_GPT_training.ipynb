{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport tqdm\nimport json","metadata":{"id":"j701WcsRp8tz","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:41:19.192154Z","iopub.execute_input":"2025-02-01T06:41:19.192488Z","iopub.status.idle":"2025-02-01T06:41:19.196746Z","shell.execute_reply.started":"2025-02-01T06:41:19.192460Z","shell.execute_reply":"2025-02-01T06:41:19.195930Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"batch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2","metadata":{"id":"ClV9HF5ep-lO","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:01.821625Z","iopub.execute_input":"2025-02-01T06:18:01.822042Z","iopub.status.idle":"2025-02-01T06:18:01.915826Z","shell.execute_reply.started":"2025-02-01T06:18:01.822014Z","shell.execute_reply":"2025-02-01T06:18:01.914839Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"torch.manual_seed(1337)","metadata":{"id":"155BIh3eqCWJ","outputId":"a6598a85-2f46-46e5-9d24-af064186fc10","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:01.917060Z","iopub.execute_input":"2025-02-01T06:18:01.917498Z","iopub.status.idle":"2025-02-01T06:18:01.941452Z","shell.execute_reply.started":"2025-02-01T06:18:01.917453Z","shell.execute_reply":"2025-02-01T06:18:01.940534Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7ccd8fb4bfb0>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"id":"_s0ZsjvGqEzC","outputId":"35f4e456-3f3e-4307-a9f0-edcecac46f77","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:01.942676Z","iopub.execute_input":"2025-02-01T06:18:01.943026Z","iopub.status.idle":"2025-02-01T06:18:03.466465Z","shell.execute_reply.started":"2025-02-01T06:18:01.942988Z","shell.execute_reply":"2025-02-01T06:18:03.465347Z"}},"outputs":[{"name":"stdout","text":"--2025-02-01 06:18:02--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n\n2025-02-01 06:18:03 (31.1 MB/s) - 'input.txt' saved [1115394/1115394]\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"with open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"id":"ntfw4MmcqIxA","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:03.468953Z","iopub.execute_input":"2025-02-01T06:18:03.469272Z","iopub.status.idle":"2025-02-01T06:18:03.475743Z","shell.execute_reply.started":"2025-02-01T06:18:03.469244Z","shell.execute_reply":"2025-02-01T06:18:03.474853Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"chars = sorted(list(set(text)))\nvocab_size = len(set(chars))\n\n# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(set(chars))}\nitos = {i: ch for i, ch in enumerate(set(chars))}\n\n# encoder: take a string, output a list of integers\ndef encode(s): return [stoi[c] for c in s]\n\n# decoder: take a list of integers, output a string\ndef decode(l): return ''.join([itos[i] for i in l])","metadata":{"id":"L7xZcajy1qaF","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:03.476868Z","iopub.execute_input":"2025-02-01T06:18:03.477238Z","iopub.status.idle":"2025-02-01T06:18:03.501108Z","shell.execute_reply.started":"2025-02-01T06:18:03.477203Z","shell.execute_reply":"2025-02-01T06:18:03.500191Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"data = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))  # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"id":"rYdIpTfjqNpt","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:03.502494Z","iopub.execute_input":"2025-02-01T06:18:03.502813Z","iopub.status.idle":"2025-02-01T06:18:03.741861Z","shell.execute_reply.started":"2025-02-01T06:18:03.502778Z","shell.execute_reply":"2025-02-01T06:18:03.740926Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:03.742972Z","iopub.execute_input":"2025-02-01T06:18:03.743256Z","iopub.status.idle":"2025-02-01T06:18:03.748861Z","shell.execute_reply.started":"2025-02-01T06:18:03.743229Z","shell.execute_reply":"2025-02-01T06:18:03.748061Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"65"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"id":"3lW6UAL2qQx8","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:03.749976Z","iopub.execute_input":"2025-02-01T06:18:03.750309Z","iopub.status.idle":"2025-02-01T06:18:03.763211Z","shell.execute_reply.started":"2025-02-01T06:18:03.750280Z","shell.execute_reply":"2025-02-01T06:18:03.762384Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in tqdm.tqdm_notebook(range(eval_iters), desc=f\"Evaluating {split}\"):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"id":"DjPGf14wqSOK","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:03.764178Z","iopub.execute_input":"2025-02-01T06:18:03.764514Z","iopub.status.idle":"2025-02-01T06:18:03.781356Z","shell.execute_reply.started":"2025-02-01T06:18:03.764490Z","shell.execute_reply":"2025-02-01T06:18:03.780445Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(\n            torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B, T, C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x)  # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei = wei.masked_fill(\n            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x)  # (B,T,hs)\n        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(\n            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        pos_emb = self.position_embedding_table(\n            torch.arange(T, device=device))  # (T,C)\n        x = tok_emb + pos_emb  # (B,T,C)\n        x = self.blocks(x)  # (B,T,C)\n        x = self.ln_f(x)  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :]  # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx","metadata":{"id":"A_CsH_DApyMw","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:07.510127Z","iopub.execute_input":"2025-02-01T06:18:07.510463Z","iopub.status.idle":"2025-02-01T06:18:07.528515Z","shell.execute_reply.started":"2025-02-01T06:18:07.510435Z","shell.execute_reply":"2025-02-01T06:18:07.527532Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model = GPTLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')","metadata":{"id":"UZrNDmhIqfRZ","outputId":"7d31331d-5cf8-45d7-df07-685297f41d95","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:10.893297Z","iopub.execute_input":"2025-02-01T06:18:10.893648Z","iopub.status.idle":"2025-02-01T06:18:11.345448Z","shell.execute_reply.started":"2025-02-01T06:18:10.893615Z","shell.execute_reply":"2025-02-01T06:18:11.344548Z"}},"outputs":[{"name":"stdout","text":"10.788929 M parameters\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in tqdm.tqdm_notebook(range(max_iters)):\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(\n            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    xb, yb = get_batch('train')\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"id":"ZvhBnRXwqbZy","outputId":"0c915890-c55b-44d5-907f-8c3ad306a8ab","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:18:12.335650Z","iopub.execute_input":"2025-02-01T06:18:12.336285Z","iopub.status.idle":"2025-02-01T06:32:25.035050Z","shell.execute_reply.started":"2025-02-01T06:18:12.336250Z","shell.execute_reply":"2025-02-01T06:32:25.034271Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/2247977947.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for iter in tqdm.tqdm_notebook(range(max_iters)):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"deebe7e91c52425bba73a6bf11ac67d9"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_23/4190024498.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for k in tqdm.tqdm_notebook(range(eval_iters), desc=f\"Evaluating {split}\"):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c62e03956bb0416b910c3938c987dfaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b8633e368cb42549724af0ac9502383"}},"metadata":{}},{"name":"stdout","text":"step 0: train loss 4.2216, val loss 4.2267\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"672ef4c2fc044613af72a2ad5aa53073"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31c9e13c7d9404695dab4e3962abcf0"}},"metadata":{}},{"name":"stdout","text":"step 500: train loss 2.2170, val loss 2.2783\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367dc90c85bd4355a8de949e96ab7686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9daec2dd92aa428eb3935a256a50692e"}},"metadata":{}},{"name":"stdout","text":"step 1000: train loss 1.6919, val loss 1.8618\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3000feebf94f2f897a14ab4834122f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95525dd81a7c43b6a69b831aa5730419"}},"metadata":{}},{"name":"stdout","text":"step 1500: train loss 1.5188, val loss 1.7200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d49baa404447a8bdfaae4e75e04f4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c396b4e033f6465ebb0824781a0bf149"}},"metadata":{}},{"name":"stdout","text":"step 2000: train loss 1.4268, val loss 1.6463\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb89812808e40bc9523d5803ff6f49e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9222c0d723cf4cb1a3d7c6fbe9466c8f"}},"metadata":{}},{"name":"stdout","text":"step 2500: train loss 1.3638, val loss 1.5929\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"301d43bc93ed49a19fc0398200487551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"571412951dc740fb90ac6a8f2cd2ba78"}},"metadata":{}},{"name":"stdout","text":"step 3000: train loss 1.3143, val loss 1.5482\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e8f7cb4bb6940e09e518807c6c4ee35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d22e8dbfee24f30b7d39b01df1a4b61"}},"metadata":{}},{"name":"stdout","text":"step 3500: train loss 1.2701, val loss 1.5373\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a63712e7ca3a43e7ac6a757b49510d26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bfa1530ac064540af13da8a117b1c85"}},"metadata":{}},{"name":"stdout","text":"step 4000: train loss 1.2435, val loss 1.5126\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5084a495c84f2eb22931de14e038d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e8ef16acff0472ba6fd14d2cbad07de"}},"metadata":{}},{"name":"stdout","text":"step 4500: train loss 1.2206, val loss 1.5130\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating train:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12c037a6641f468390c5e9d04c338712"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating val:   0%|          | 0/200 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9782e866fef49618d7298dd08f4d553"}},"metadata":{}},{"name":"stdout","text":"step 4999: train loss 1.1921, val loss 1.4843\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"context = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))","metadata":{"id":"R3rCPe1_qpu2","outputId":"5eebf7bb-e456-4ca0-caea-919cad3f6756","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T07:19:47.636759Z","iopub.execute_input":"2025-02-01T07:19:47.637753Z","iopub.status.idle":"2025-02-01T07:20:06.365149Z","shell.execute_reply.started":"2025-02-01T07:19:47.637700Z","shell.execute_reply":"2025-02-01T07:20:06.364304Z"}},"outputs":[{"name":"stdout","text":"For Johhn, and the perceament.\nThey ragent purrue the steeds and themselves,\nAnd put under blood us; bethought your wonders redur'd\nMade my heart's honour at the shaert\nAnd succession the causels\nWith a her centuring fair mind. What you please, my lies\nMost in that of yourselves with men-majourng\nMerely hath one you with thunders' forward,\nThrought's coats, of the knessly death.\nUf you shall I life absetter, for your news\nShall detire you be, your countrymen\nOf me hare prayershed as on the enmy earr eyes.\nEdismen and kisses and his wimorious, if so ready.\n\nFirst Salkningman:\nSo dire, good man, rume his take; one word\nShall were by her desperate in our goodly.\n\nSTANLEY:\nGo, my lord, beseech your quisiness: for we,\nThank you without an the old in fue\nFor far the child, yet I be displited;\nI clay you now: I'll keep at.\n\nSICINIUS:\nSee, she is he; that it loves men.\n\nFirst Senator:\nMy master to harm gracious Lord Caius;--\n\nLehind of Solenho, let me to be pay'd.\nHere command, Beamingbroke: do\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"prompt = \"\"\"Write a scene about Romeo arguing with Juliet.\nROMEO:\"\"\"\n\ninput = torch.tensor([encode(prompt)], dtype=torch.long, device=device)\nprint(decode(m.generate(input, max_new_tokens=1000)[0].tolist()))","metadata":{"id":"qZFV2aVZmOqE","outputId":"a99f9899-7ef9-43a8-fbd7-4d2b0aff3bca","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:34:04.650204Z","iopub.execute_input":"2025-02-01T06:34:04.650589Z","iopub.status.idle":"2025-02-01T06:34:23.082806Z","shell.execute_reply.started":"2025-02-01T06:34:04.650556Z","shell.execute_reply":"2025-02-01T06:34:23.081936Z"}},"outputs":[{"name":"stdout","text":"Write a scene about Romeo arguing with Juliet.\nROMEO:\n'Beseech you, toy! here is the argins of your old bosom,\nNot his lexire yung acking; then, if I had win\nYet rather obedience and grimas' blood\nA suitor word, indeed, my lord,\nGrose and she has liately sped's death.\n\nESCALUS:\nBear for a  tworld; here's our tongue.\n\nESCALUS:\nNow, sir, if, we shall bleed in holy true.\n3 KING HENRY VI\n\nDUCHESS OF YORK:\nBasta; what, of YORK:\nWill you revien your put mot,\nThat do do men as, say you were tutern men,\nWhen it she that seems suffers against us tears for grail,\nWere brew the husburns for that they even could be theirs.\nThe tribb of have they markled to the compie,\nOf blood? well sayst never grating piece with slander?\n\nSecond Murderer:\nHe cames joy stand and hath a Capuret reserve.\n\nThird Servingman:\nHappite them from Aufit, or I marry:\nThen, whom what noUselves thee?\n\nNORTYRUCHBY:\nAh, nor a liver belihve no deed,\nWhen they soulst was upon our dain swering.\n\nROMEO:\nO this armor!\n\nNurse:\nWhy, what is the rarlethed many guad there the come,\nOr no \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"torch.save(m.state_dict(), 'GPT_model_char.pt')","metadata":{"id":"nfeS4V4g03Jl","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:35:14.663865Z","iopub.execute_input":"2025-02-01T06:35:14.664481Z","iopub.status.idle":"2025-02-01T06:35:14.758876Z","shell.execute_reply.started":"2025-02-01T06:35:14.664444Z","shell.execute_reply":"2025-02-01T06:35:14.758130Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"batch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    \"vocab_size\": vocab_size,\n    \"batch_size\": batch_size,\n    \"block_size\": block_size,\n    \"max_iters\": 5000,\n    \"eval_interval\": 500,\n    \"learning_rate\": 3e-4,\n    \"device\": device,\n    \"eval_iters\": 200,\n    \"n_embd\": 384,\n    \"n_head\": 6,\n    \"n_layer\": 6,\n    \"dropout\": 0.2,\n    \"encode\": stoi,\n    \"decode\": itos\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:40:25.947055Z","iopub.execute_input":"2025-02-01T06:40:25.947664Z","iopub.status.idle":"2025-02-01T06:40:25.952142Z","shell.execute_reply.started":"2025-02-01T06:40:25.947628Z","shell.execute_reply":"2025-02-01T06:40:25.951212Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"with open('config.json', 'w') as f:\n    json.dump(config, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T06:42:02.323446Z","iopub.execute_input":"2025-02-01T06:42:02.323996Z","iopub.status.idle":"2025-02-01T06:42:02.328939Z","shell.execute_reply.started":"2025-02-01T06:42:02.323962Z","shell.execute_reply":"2025-02-01T06:42:02.328139Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}